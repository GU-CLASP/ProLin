#+OPTIONS: toc:nil ':t ":t
#+LATEX_CLASS: article

#+LATEX_HEADER: \pdfpagewidth=8.5in
#+LATEX_HEADER: \pdfpageheight=11in
#+LATEX_HEADER: \usepackage{ijcai20}
#+LATEX_HEADER: \usepackage{times}
#+LATEX_HEADER: \usepackage{soul}
#+LATEX_HEADER: \usepackage{url}
# FIXME: #+LATEX_HEADER: \usepackage[hidelinks]{hyperref}
# FIXME: #+LATEX_HEADER: \usepackage{natbib}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[small]{caption}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \urlstyle{same}

# guidelines: https://www.ijcai.org/authors_kit

#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \newcommand{\ttr}[1]{\left[\begin{array}{lcl}#1\end{array}\right]}
#+LATEX_HEADER: \newcommand{\tf}[2]{\mathrm{#1} & : & \mathit{#2}\\}
#+LATEX_HEADER: \newcommand{\rf}[2]{\mathrm{#1} & = & \mathit{#2}\\}
#+LATEX_HEADER: \newcommand{\mf}[3]{\mathrm{#1=#2} & : & \mathit{#3}\\}
#+LATEX_HEADER: \newcommand{\type}[1]{$\mathit{#1}$}


#+TITLE: Questions and clarifications with linear logic
#+AUTHOR:

\begin{abstract}
In this paper we propose an account for dialogue coherence using Linear Logic. We focus our study on the range of things that can be potentially clarified in dialogue, and argue that they can be represented as meta-variables. 
\end{abstract}


* Introduction
A key aspect of dialogue systems design is the coherence of system’s
responses.  In this respect, key component of a dialogue system is the
dialogue manager, which selects appropriate system actions depending
on the current state and the external context.

Two families of approaches to dialogue management can be considered:
hand-crafted dialogue strategies
citep:allen1995trains,larsson2002issue,jokinen2009constructive and
statistical modelling of dialogue
citep:rieser2011reinforcement,young2010hidden. Hand-crafted strategies
range from finite-state machines and slot-filling to more complex
dialogue planning and logical inference rules. Statistical models help
to contend with the uncertainty that arises from noisy signals that
arise from speech recognition and other sensors. 

End-to-end systems (they don't have dialogue manager as an explicit
component) [fn::TBD refs, incl recent Facebook paper] are mostly
focused on chit-chat dialogues, where coherence plays a crucial role
too. [fn::TBD what are their struggles?]

Although there has been a lot of development in dialogue systems in
recent years, only a few approaches to dialogue management reflect
advancements in /dialogue theory/
citep:allen1995trains,ginzburg1996interrogatives,poesio1997conversational,larsson2002issue,
and there has not been much progress in this respect since the early
2000s. Our aim is to closely integrate dialogue systems with work in
theoretical semantics/pragmatics of dialogue.


- summarise dialogue system with Linear logic cite:dixon2009plans
- references on clarification, why do we need to have clarifiable things, meta-var for places that can be clarified
  - also mention that it is a way to recover from / prevent ASR/NLU errors

* Background

** Dialogue management                                                 :Vlad:
*** KoS
TODO: we are not implementing Kos here, just use something from it


KoS (not an acronym) citep:ginzburg2012interactive provides among the
most detailed theoretical treatments of domain general conversational
relevance citep:ginzburg2012interactive, especially for query
responses---see citet:purver-rlc06 on Clarification Requests,
citet:lupkowski2017query for a general account---and this ties into
the KoS treatment of non sentential utterances, again a domain crucial
for naturalistic dialogue systems and where KoS has among the most
detailed analyses citep:fgl07,ginzburg2012interactive.[fn::TBD DS/TTR,
incrementality?]

In KoS (and other dynamic approaches to meaning), language is compared
to a game, containing players (interlocutors), goals and rules. KoS
represent language interaction by representing the dynamically
changing context. The meaning of an utterance is how it changes the
context. Compared to most approaches
[[citep:roberts2012information][e.g.::]], which represent a single context
for both dialogue participants), KoS keeps a separate representation
for each participant, using the /Dialogue Game Board/
(DGB). DGBs represent the information states of the participants, and
comprise a private part and the dialogue gameboard that represents
information arising from publicized interactions. It tracks, at the
very least, shared assumptions/visual space, moves (= utterances, form
and content), and questions under discussion.

KoS is based on the formalism of Type Theory with Records (TTR). There
has been a wide range of work in this formalism which includes the
modelling of intentionality and mental attitudes citep:cooper-rlc,
generalised quantifiers citep:cooper-gq13, co-predication and dot
types in lexical innovation, frame semantics for temporal reasoning,
reasoning in hypothetical contexts citep:cooper-lacl11, spatial
reasoning citep:dobnik2017interfacing, enthymematic reasoning
citep:ellen-aisb, clarification requests
citep:purver-rlc06,ginzburg2012interactive, negation
citep:cooper2012negative, non-sentential utterance resolution
citep:fgl07,ginzburg2012interactive and iconic gesture
citep:lucking16.

*** Information state update approach
In this work we are employing information-state update (ISU) approach,
following
citet:traum1999model,larsson2002issue,ginzburg2015understanding. In
this view we present the information available to each participant of
the dialogue (either a human or an artificial agent) in a rich
information state. Being rich entails that the information state
contains hierarchy of facts, including the ones that are thought to be
shared and the ones that have not been yet publicised. Let’s consider
the following example of the information state of a dialogue system,
expressed in the TTR notation (Type Theory with Records, for further
info see [fn::TBD fixed ref]):
\begin{equation}
\ttr{
\rf{private}{\ttr{\rf{tt_1}{TT(Bus52,0,Skogome,Götaplatsen)}
                  \rf{tt_2}{TT(Bus18,1,Johanneberg,Götaplatsen)}}}
\rf{public}{\ttr{\rf{latestUtterance}{Ask(U,Question(\lambda t.TT(b,t,d,Götaplatsen)))}}}}
\end{equation}
What is expressed here is that dialogue system was just asked by a
user ($U$) a question like ‘When is there a bus from Götaplatsen’ (the
underspecified[fn::TBD more about underspecification] variables $b$, $t$ and $d$ correspond to bus number,
time and destination respectively). This information is
/public/[fn::Later on, following citet:ginzburg2015understanding we will
denote the public part of the information state as Dialogue Gameboard
(DGB).] and available to a user as well. What is private is the
knowledge of the timetable ($TT$) available to the system and not yet
made public, hence it is stored in the /private/ part of the information
state.

Let’s now consider the /update/, another essential component of ISU. In
this case, we will rely on the set of rules, that will govern the
updates. citet:ginzburg2015understanding defines one of the most basic
rules -- the rule of QUD-incrementation -- the procedure of updating
the current set of questions under discussions (QUD) if the latest
utterance is comprised of asking the question. This operation is
salient to a user and therefore it constitutes the update of the
public part of the information state.

#+BEGIN_SRC python :exports code
if public.latestUtterance == Ask(U, Question(x)):
    push Question(x) into public.QUD
#+END_SRC
The updated state will look as follows:
\begin{equation}
\ttr{
\rf{private}{\ttr{\rf{tt_1}{TT(Bus52,0,Skogome,Götaplatsen)}
                  \rf{tt_2}{TT(Bus18,1,Johanneberg,Götaplatsen)}}}
\rf{public}{\ttr{\rf{latestUtterance}{Ask(U,Question(\lambda t.TT(b,t,d,Götaplatsen)))}
              \rf{QUD}{set(Question(\lambda t.TT(b,t,d,Götaplatsen))}}}}
\end{equation}

The main benefit of using rich representation of information state
with underspecified components is to be able to address wide range of
clarifications from both parties. This is especially beneficial in
case of automatic speech recognition or natural language understanding
errors. But even put the errors aside, we can also consider topically
relevant follow-up questions by the system, e.g. `What bus?', or
contributions when user provides more information than they were
asked, e.g. `Bus 18 to Skogome'.
 
** Clarification requests                                                :Vlad:
** Proof search as a programming language

The prevailing tradition in formal semantics (TODO:citations) is to
represent (declarative) statements as propositions, formalized in an
underlying logic (such as propositional logic, first-order logic,
etc.).

In particular, in linguistic theories based on intuitionistic logic
(such as TTR), true statements corresponds to propositions which admit
a proof.

There is a long history (TODO:citations) of using proof search as a
declarative programming paradigm.  In the most abstract sense, the
programmer specifies /axioms/ and /rules of inference/ which model
their application domain. Typically such a system of axioms and rules
represent a database of facts. For example, the axiom Leave 55
Götaplatsen 11.50 can model the fact that bus 55 leaves from
Götaplatsen at 11:30. The rule Leave x Gotaplatsen y -> Arrive x
CentralStationen (y+0.75) can represent travelling times on a certain
line.

Then, the user may define a query (or goal) as
a logical formula. The system can then search for a proof of goal as a
way to query the database of facts. The most useful case are goals
which contain /metavariables/. For example, the goal "Leave x
Götaplatsen y" corresponds to a request to list all the buses leaving
from Götaplatsen (as x) together with their departure time (as y).


*** Linear logic
Typically, and in particular in the archetypal logic programming
language prolog, axioms and rules are expressed within the general
framework of first order logic. However, several authors (cite:) have
proposed to use instead linear logic. For our purpose, the crucial
feature of linear logic is that facts may be used /only once/. For example, could have a rule
Is x Gotaplatsen y -o Is x CentralStationen (y+0.75)
Consequently, after firing the above rule, the premiss (Is x Gotaplatsen y) becomes unavailable for other rules.
Thereby the linear arrow (-o) can be used to conveniently model that a bus cannot be at two places.

Thus, the hypotheses available for proof search correspond to the
/state/ of the system. In our application they will correspond to the
state of the dialog (roughly speaking the dialog game board).

This way, the firing of a linear rule corresponds to an /action/ of an
agent, and a complete proof corresponds to a /scenario/, i.e. a
sequence of actions, possibly involving action from several agents.
However, the state of the system (typically in the literature and in
this paper as well), represent the (mental) state of a /single/
agent. A scenario is concieved as the plan of a single agent as to how
a dialogue develops. Scenarii can be realised as an actual sequence of
exchange of messages.  That is, an action can result in sending a
message to the outside world (in the form of speech, movement,
etc.). Conversely, events happenning in the outside world can result
in updates of the state (through a model of the perceptory subsystem).

In an actual dialogue, the scenario is therefore suspended after every
interaction, and the state represent the actual mental state of the
agent which is modeleled.

TODO: examples!

It is important to note that we will not forego the unrestricted
(i.e. non-linear) implication (->). Rather, both implication will
co-exist in our implementation, thus we can represent simultaneously
transient facts, or states, (introduced by the linear arrow) and
immuable facts (introduced by the unrestricted arrow).

*** Metavariables and unification

In prolog-like languages, metavariables play the role of unknowns,
whose value can become fixed for a goal to be reached.

In the context of linear-logic proof search, this means that, at any
point in the scenario, state can refer to metavariables.

(Example) 

In this situation, metavariables represent a certain amount of
flexibility in the scenario: /so far/ the scenario works for any value
which could be assigned to the metavariable. In this paper we explore
the potential of using metavariables in this context.

* TODO Theory and running example                                        :JP:

** Question-answering with metavariables

A first use for metavariables is to represent the requested answer.

We choose here to represent a question by a predicate over a type A.

A : Type
P : A  -> Prop

Examples:

where do you live?
do you live in paris?

In the state of the agent, if the value of the requested answer is
represented as a metavariable x, then the question can be represented as:

Q x (P x)

That is, the pending question is a pair of a metavariable x and a proposition
where x occurs.

We stress that P x is /not/ part of the state of the agent yet, rather
the fact that the above question is /under discussion/ is a fact:

QUD (Q x (P x))

Resolving a question can be done by communicating an answer. An answer
to a question (A : Type; P : A -> Type) is a pair of an element x:A
and a /proof/ that P x holds.

Examples.

Therefore, one way to resolve a question is by the following rule:

∀ x p'. Stated p' -> QUD (Q x p') -o p'

That is, if it was stated P', and P' was under discussion, then P'
holds.  Indeed, this requires that the statement and the posed
question /unify/. Examples:

"I live in paris" answers both questions "where do you live" and "do
you live in paris", but, not, for example "what time is it?".

(Formalize examples)

However, in layman terms,one would consider the question answered only
if the answer is "unique". For example, "I live somewhere" is not a
valid answer to "where do you live". That is, if "somewhere" is
represented by a metavariable, then the answer is not fitting.

To be able to represent this, we extend linear logic with an operator !-> :

TODO

With this, we are ready to state our key idea:

meta-variable stand in for any piece of information which is left for
interpretation

** Clarification requests

In this section we consider the repercussions of our key idea in the
context of clarification requests.

Assume a two-place predicate /Eat/ with agent as first argument and
object as second argument. The phrase "John eats an apple" could then
be represented as /Eat(John,Apple)/. According to our theory, one can
then represent the phrase "John eats" as /Eat(John,x)/, with /x/ being
a metavariable.

Assume now a system with the state:

/Eat(John,Apple)/

Then the question "what does John eat", represented as /(Q x
(Eat(John,x)))/, can be answered.  From the point of view of modelling
with linear logic, we could attempt to model the answering by the
rule:

(a : Type) -> (x : a) -> (p : Prop) -> QUD (Q x p) -> P ⊸ (P ⊗ Answer x (Q x P))

The above states that, if $x$ makes the proposion $p$ true (more
precisely, provable) then it is valid to answer $x$ if $p$ is under
discussion. However, there is an issue with the above rule: if $x$ is
/not unique/, then one would not consider $x$ a suitable answer. Indeed,
assume instead that the system is in the state:

/Eat(John,x)/

Then the question cannot be answered, because /x/ stands for some
unknown thing. The proper answer is then "I do not know".

Hence, we introduce another type-former /(x : A) !-> B/. As for $(x :
A) -> B$, it introduces the metavariable $x$. However, the rule fires
only when $x$ is made /ground/ and /unique/ by matching the rule. That
is, it won't match in the previous example, because the answer is not
ground (it contains unknowns). Additionally, it won't match if the
state of the system is composed of the two hypotheses
/Eat(John,Apple)/ and /Eat(John,Orange)/: the answer is not unique. 

Thus, the rule for answering can be written:

(a : Type) -> (x : a) !-> (p : Prop) -> QUD (Q x p) -> P ⊸ (P ⊗ Answer x (Q x P))

For the above example: A proper answer could be "An apple and an orange" or "An apple or an
orange". However we consider here a third possibility: instead of
answering, the agent can issue a clarification request (TODO: is this
reasonable? When ... etc.)

To illustrate, consider the question "What is being eaten?"
represented as /Q x (Eat(y,x))/.  with the state /Eat(John,Apple)/ and
/Eat(Mary,Apple)/. Then the agent can unambguously answer "An apple":
even if we do not know who we're talking about, it does not matter:
only an apple is being eaten. However, If the state is
/Eat(John,Apple)/ and /Eat(Mary,Orange)/, then a probable answer would
be a /clarification request/, namely "eaten by who?".

To detect situations where a clarification request can be issued, we can use the following rule:

(a : Type) -> (x : a) ?-> (p : Prop) -> QUD (Q x p) -> P ⊸ (P ⊗ CR)

The conditions are similar to that of the answering rule. The
principal difference is the use of the ?-> operator, which conditions
on a metavariable which remains not (fully) ground, or which can be
unified to several ground terms --- the opposite of the !-> operator.

We can then turn our attention to the formulation of this clarification request.
It is itself a question, and has a tricky representation:

Q z (z = y)

That is, the question is asking about some aspect which was left
implicit in the original question (what is being eaten). In our terms,
it must refer to the (implicit) metavariable which the original
question included (y). After getting an answer, (say "Mary"), z will
be bound to a ground term, and, in turn, the fact z=y will ensure that
y becomes ground. This means the original question will, by
unification, become Q x (Eat(Mary,x)), and it can be unambiguously
answered using the /canAnswer/ rule. We note that the logical form of
the question (z such that (z=y)) is typically realised in a
complicated way. In our example, it could be "eaten by who"; echoing
part of the original question and assuming cooperative communication
so that the questioner properly relates the clarification request to
the implicits of the original questions. (In sec. TODO we show other examples.)

In practice, the form of clarification questions will greatly vary depending on the context.

The above suposes a clear-cut distinction: if an answer is unique, it
is given; otherwise a clarification request is issued. However,
answers could simply be exhaustive ("An appler or an orange").  If the
original questioners are unhappy with the ambiguity, they are free to
issue more precise questions. In practice, one can easily imagine an
ambiguity threshold after which clarification requests are
preferred. In the simplest form, this ambiguity threshold could be
expressed by the length of the answer. In our example, if one has to
list, say, 20 different types of food, it is easy to imagine that the
answer won't be fully given. In fact, this question can be the topic
of an experimental study.

** Extras
In the linear logic implementation we treat the information /state as a
set [?] of /resources/ that can be queried and/or used. This is the
basic set of operators that constitute our implementation and that
makes it different from cite:dixon2009plans. [fn::TBD examples for each]
- Query (~X -* Y~) :: the resource ~X~ is queried and if the result is
  positive the resource ~Y~ is produced.
- Query for uniqueness (~X !-> Y~) :: the resource ~X~ is queried for
  uniqueness and if the result is positive the resource ~Y~ is produced.
- Linear implication (~X -o Y~) :: the resource ~X~ is queried and if the
  result is possible, ~X~ is consumed and the resource ~Y~ is
  produced.
- Conjunction of the results (~[_:: X; _:: Y]~) :: this is a way to produce
  both ~X~ and ~Y~ as the result of applying the given rule.

As an example, we can show how the rule for /QUD-incrementation/ from
citet:ginzburg2015understanding can be formulated in this terms. Here
we consider the dialogue between interlocutors /A/ and /B/, when /A/ asks
/B/[fn::Here we omit addressees as the conversation is only two-party.]
a question /Q/. The question /Q/ just have been posed and therefore has
appeared on the DGBs of both /A/ and /B/ as the latest ~Ask~ move
(~LatestMove~).
#+BEGIN_SRC sh :exports code
-- context
_ :: DGB A (LatestMove (Ask A Q));
_ :: DGB B (LatestMove (Ask A Q));
#+END_SRC

Now we can define our update rule that act on the contextual resources:
#+BEGIN_SRC
_ : (q : Question) -> (x y : User) ->
    DGB x (LatestMove (Ask y q)) -o DGB x (QUD q);
#+END_SRC
Here, for any interlocutor, her ~LatestMove~ asking a question is
consumed and her ~QUD~ is updated with the question from the ~Ask~ move.


- explain the system
- explain the extension with unique 
- type checking?

* Evaluation/Discussion/Future work
- discussing the corrections
- discuss the clarification requests in a more specific way: we can always redefine the referent
- in dialogue systems meta-variables are always subject to clarification and correction (substitution)
- dependencies between questions (who killed bill -> who was around?)

* References :ignore:
bibliographystyle:named
bibliography:lacatoda.bib
