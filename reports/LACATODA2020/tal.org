#+OPTIONS: toc:nil ':t ":t 
#+LATEX_CLASS: article-hermes_french
#+LATEX_HEADER: \usepackage[labelfont=bf,textfont=it,labelsep=period,justification=raggedright,singlelinecheck=false]{caption}

#+LATEX_HEADER: %include polycode.fmt
#+LATEX_HEADER: %format !-> = "\rightarrow_{!}"
#+LATEX_HEADER: %format ?-> = "\rightarrow_{?}"
#+LATEX_HEADER: %format . = "."
#+LATEX_HEADER: %let operator = "."
#+LATEX_HEADER: \usepackage{soul}
#+LATEX_HEADER: \usepackage{url}
#+LATEX_HEADER: \usepackage{newunicodechar}
#+LATEX_HEADER: \input{newunicodedefs}
# #+LATEX_HEADER: \usepackage{natbib}
# Natbib-like commands for harvard.sty:
#+LATEX_HEADER: \newcommand\citep[2][]{\cite[#1]{#2}}
#+LATEX_HEADER: \newcommand\citet[2][]{\citeasnoun[#1]{#2}}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \urlstyle{same}
#+LATEX_HEADER: \usepackage{makecell}
#+LATEX_HEADER: \usepackage{rotating}

#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \newcommand{\ttr}[1]{\left[\begin{array}{lcl}#1\end{array}\right]}
#+LATEX_HEADER: \newcommand{\tf}[2]{\mathrm{#1} & : & \mathit{#2}\\}
#+LATEX_HEADER: \newcommand{\rf}[2]{\mathrm{#1} & = & \mathit{#2}\\}
#+LATEX_HEADER: \newcommand{\mf}[3]{\mathrm{#1=#2} & : & \mathit{#3}\\}
#+LATEX_HEADER: \newcommand{\type}[1]{$\mathit{#1}$}
#+LATEX_HEADER: \newcommand{\jg}[1]{\noindent \textcolor{blue}{\textbf{\emph{[jg:  #1]}}}}

#+TITLE: Dummy, see below.

#+SUBTITLE: \title[linear logic, metavariables and Q/A]{Dialogue management with linear logic: the role of metavariables in questions and clarifications}
#+AUTHOR: Anonymous
#+latex_header: \input{tal-preamble.tex}


* Introduction

# Establish the field. What is the line of work? (references, etc.)
# Vlad

# Identify a gap.

- Semantic gap: the ideas are expressed in a formalism which is not the
  best match.

  (For example the TTR merge is a hack! 1. ad-hoc addition to
  type-theory. 2. destructive updates, deletes, etc.)

- Implementation gap.

- Who cares about this anyway and what do they see as important?

# How we plan to fill this gap?

KEY IDEA: meta-variable stand in for any piece of information which is left for
interpretation

# What do we get?



* Background

** Dialogue management                                                 :Vlad:
A key aspect of dialogue systems design is the coherence of system’s
responses.  In this respect, a key component of a dialogue system is the
dialogue manager, which selects appropriate system actions depending
on the current state and the external context.

Two families of approaches to dialogue management can be considered:
hand-crafted dialogue strategies
cite:allen1995trains,larsson2002issue,jokinen2009constructive and
statistical modelling of dialogue
cite:rieser2011reinforcement,young2010hidden. Frameworks for
hand-crafted strategies range from finite-state machines and
form-filling to more complex dialogue planning and logical inference
systems. Statistical models help to contend with the uncertainty that
arises in human interaction; from noisy signals from speech
recognition and other sensors to pragmatic ambiguities.

End-to-end systems that do not specify a dialogue manager as an
explicit component have gained lots of attention recently (e.g.,
cite:roller2020recipes). Although most of them are focused on
chit-chat dialogues, coherence plays a crucial role there
too. Typically the main issues associated with such systems are
related to memory limitations which cause repetition, contradiction
and forgetfulness. Having a policy for dialogue coherence would be
beneficial for such systems.

Although there has been a lot of development in dialogue systems in
recent years, only a few approaches reflect advancements in /dialogue
theory/. Our aim is to closely integrate dialogue systems with work in
theoretical semantics and pragmatics of dialogue. This field has provided accounts for linguistic phenomena intrinsic to dialogue such as non-sentential utterances cite:schlangen-diss,flg08,ginzburg2012interactive, clarification requests cite:purcer-rlc06,ginzburg2012interactive and self-repair cite:gfs-sp,houghpurver-disfl, where the resolution is intuitively tied to the coherence of what is being said.

*** Information state update approach
In this work we are employing an information-state update (ISU) approach,
following several authors, including
citet:traum1999model, citet:larsson2002issue, and citet:ginzburg2012interactive. In this
view we present the information available to each participant of the
dialogue (either a human or an artificial agent) in a rich information
state. Being rich entails that the information state contains
hierarchy of facts, including the ones that are thought to be shared
and the ones that have not been yet publicised.[fn::TBD consider if we
need this] As an illustration [ref:eq:isu-before-update] is a rough
exemplification the information state of a dialogue system, in TTR
notation.
\begin{equation}\label{eq:isu-before-update}
\def\arraystretch{1.5}
\setlength{\arraycolsep}{1pt}
\ttr{
\rf{private}{\ttr{\rf{tt_1}{TT(Bus52,0,Sko,Got)}
                  \rf{tt_2}{TT(Bus18,1,Joh,Got)}}}
\rf{public}{\ttr{\rf{LU}{Ask(U,Question(\lambda t.TT(b,t,d,Got)))}
                 \rf{QUD}{set()}}}}
\end{equation}

What is expressed here is that dialogue system was just asked (in the
field |public.LU| -- latest utterance) by a user ($U$) a question like
"When is there a bus from Gotaplatsen" (the underspecified[fn::TBD
more about underspecification] variables $b$, $t$ and $d$ correspond
to bus number, time and destination respectively). This information is
/public/[fn::Later on, following cite:ginzburg2012interactive we will
denote the public part of the information state as the Dialogue
Gameboard (DGB).] and available to a user as well. What is private is
the knowledge of the timetable ($TT$) available to the system and not
yet made public, hence it is stored in the /private/ part of the
information state.

Let’s now consider the /update/, another essential component of ISU. In
this case, we will rely on a set of rules, that will govern the
updates. citet:ginzburg2012interactive defines one of the most basic
rules -- the rule of QUD-incrementation -- the procedure of updating
the current set of questions under discussions (|QUD|) if the latest
utterance (|LU|) is a question. This operation is salient to a user
and therefore it constitutes the update of the public part of the
information state.

#+BEGIN_code
if public.LU = Ask(U, Question(x)):
    push Question(x) into public.QUD
#+END_code
The updated state will look as follows:
\begin{equation}
\def\arraystretch{1.5}
\setlength{\arraycolsep}{1pt}
\ttr{
\rf{private}{\ttr{\rf{tt_1}{TT(Bus52,0,Sko,Got)}
                  \rf{tt_2}{TT(Bus18,1,Joh,Got)}}}
\rf{public}{\ttr{\rf{LU}{Ask(U,Question(\lambda t.TT(b,t,d,Got)))}
              \rf{QUD}{set(Question(\lambda t.TT(b,t,d,Got))}}}}
\end{equation}

The main benefit of using rich representation of information state
with underspecified components is to be able to address a wide range
of clarifications from both parties. This is especially beneficial in
case of automatic speech recognition or natural language understanding
errors. But even putting such errors aside, we can also consider
topically relevant follow-up questions by the system, e.g. "What
bus?", or contributions when the user provides more information than
they were asked, e.g. "Bus 18 to Skogome".
 
*** KoS
KoS (not an acronym) cite:ginzburg2012interactive provides one of the
most detailed theoretical treatments of domain general conversational
relevance, especially for query
responses---see citet:purver-rlc06 on Clarification Requests,
cite:lupkowski2017query for a general account---and this ties into
the KoS treatment of non sentential utterances, again a domain crucial
for naturalistic dialogue systems and where KoS has among the most
detailed analyses cite:fgl07,ginzburg2012interactive.[fn::TBD DS/TTR,
incrementality?]

In KoS (and other dynamic approaches to meaning), language is compared
to a game, containing players (interlocutors), goals and rules. KoS
represent language interaction by representing the dynamically
changing context. The meaning of an utterance is how it changes the
context. Compared to most approaches
[[cite:roberts2012information][e.g.::]], which represent a single context
for both dialogue participants), KoS keeps a separate representation
for each participant, using the /Dialogue Game Board/
(DGB). DGBs represent the information states of the participants, and
comprise a private part and the dialogue gameboard that represents
information arising from publicized interactions. It tracks, at the
very least, shared assumptions/visual space, moves (= utterances, form
and content), and questions under discussion.

KoS is based on the formalism of Type Theory with Records (TTR). There
has been a wide range of work in this formalism which includes the
modelling of intentionality and mental attitudes cite:cooper-rlc,
generalised quantifiers cite:cooper-gq13, co-predication and dot types
in lexical innovation, frame semantics for temporal reasoning,
reasoning in hypothetical contexts cite:cooper-lacl11, spatial
reasoning cite:dobnik2017interfacing, enthymematic reasoning
cite:ellen-aisb, clarification requests
cite:purver-rlc06,ginzburg2012interactive, negation
cite:cooper2012negative, non-sentential utterance resolution
cite:fgl07,ginzburg2012interactive and iconic gesture cite:lucking16.

** Clarification requests                                                :Vlad:
- what are CRs
- why do they exist?
- how frequent are they?
- what gets clarified
- why it is crucial for SDS:
  - why fine-grained
  - user responses with open vocabulary ASR

** Proof search as a programming language

The prevailing tradition in formal semantics (TODO:citations) is to
represent (declarative) statements as propositions, formalized in an
underlying logic (often first⊸rder logic).

In particular, in linguistic theories based on intuitionistic logic
(such as TTR), true statements corresponds to propositions which admit
a proof.

There is a long history (TODO:cite prolog seminal paper?) of using
proof search as a declarative programming paradigm.  In the most
abstract sense, the programmer specifies /axioms/ and /rules of inference/
which model their application domain. Typically such a system of
axioms and rules represents a database of facts. For example, the
axiom |(Leave 55 Gotaplatsen 11.50)| can model the fact that bus 55
leaves from Götaplatsen at 11:50. The rule |(Leave x Gotaplatsen y ->
Arrive x CentralStationen (y+0.75))| can represent travelling times on
a certain line.

Then, the user may define a query (or goal) as
a logical formula. The system can then search for a proof of a goal as a
way to query the database of facts. In the most useful cases, goals
contain /metavariables/[fn::here, we use the convention that metavariables are lowercase letters.]. For example, the goal |(Leave x
Götaplatsen y)| corresponds to a request to list all the buses leaving
from Götaplatsen (as |x|) together with their departure time (as |y|).


*** Linear logic
Typically, and in particular in the archetypal logic programming
language prolog (TODO citation), axioms and rules are expressed within the general
framework of first order logic. However, several authors
cite:dixon2009plans,martens2015programming have proposed to use
linear logic cite:girard1995linear instead. For our purpose, the
crucial feature of linear logic is that facts may be used /only
once/. For example, one could have a rule |IsAt x Gotaplatsen y ⊸ IsAt x
CentralStationen (y+0.75)|. Consequently, after firing the above rule,
the premiss |(Is x Gotaplatsen y)| becomes unavailable for any other rule.
Thereby the linear arrow |⊸| can be used to conveniently model that a
bus cannot be at two places simultaneously.

Thus, the hypotheses available for proof search correspond to the
/state/ of the system. In our application they will correspond to the
/information state/ of the dialog participant.

We note that in linear logic, facts (hypotheses) to not come in a
hierarchy (TODO as above). Either we have a fact, or we don't. However
in second order variants of intuitionistic logic, like the one we use,
one can conveniently wrap propositions in constructors, to indicate
that they come with a qualification. For example, we can write |Unsure
P| to indicate that the proposition |P| may hold (for example if
clarification is required).

This way, the firing of a linear rule corresponds to an /action/ of an
agent, and a complete proof corresponds to a /scenario/, i.e. a sequence
of actions, possibly involving action from several agents.  However,
the information state (typically in the literature and in this paper
as well), corresponds to the state of a /single/ agent. Thus, a scenario
is conceived as a sequence of actions and updates of the information
state of a single agent $a$, even though such actions can be
attributed to any other dialogue participant $b$. (That is, they are
$a$'s representation of actions of $b$.)  Scenaria can be realised as
a sequence of actual actions and updates. That is, an action can
result in sending a message to the outside world (in the form of
speech, movement, etc.). Conversely, events happening in the outside
world can result in updates of the information state (through a model
of the perceptory subsystem).

In an actual dialogue, the scenario is therefore suspended between
every interaction, and the state represents the current mental state
of the agent which is modelled.[fn::possibly remove this sentence]

TODO: forward reference to example.

It is important to note that we will not forego the unrestricted
(i.e. non-linear) implication (|->|). Rather, both implications will
co-exist in our implementation, thus we can represent simultaneously
transient facts, or states, (introduced by the linear arrow) and
immutable facts (introduced by the unrestricted arrow).

*** Metavariables and unification

In prolog-like languages, metavariables play the role of unknowns,
whose value can become fixed for a goal to be reached.

In the context of linear-logic proof search, this means that, at any
point in the scenario, state can refer to metavariables.

TODO: forward reference to example.

In this situation, metavariables represent a certain amount of
flexibility in the scenario: /so far/ the scenario works for any value
which could be assigned to the metavariable. In this paper we explore
the potential of using metavariables in this context.

* Questions and clarifications

** Question-answering with metavariables

A first use for metavariables is to represent the requested answer of a question.

In this paper, we represent a question by a predicate P over a type A. That is, using a typed intuitionistic logic:
#+BEGIN_code
A  : Type
P  : A  -> Prop
#+END_code

The intent of the question is to find out about a value $x$ of type
$A$ which makes $P x$ true. We show several examples in table
ref:tbl:qa-ex.  It is worth stressing that the type $A$ can be large
(for example asking for any location) or as small as a boolean (if one
requires a simple yes/no answer).  We note in passing that, typically,
polar questions can be answered not just by a boolean but by
qualifing the predicate in question (Table ref:tbl:qa-ex, last two
rows).  In this instance |A = Prop -> Prop|. Also, a simple "no"
answer can be ambiguous when the question is negative cite:cooper2012negative.


\begin{sidewaystable} %[htbp]
\begin{tabular}{lllll}
utterance & A & P & a\\
\hline
Where does John live?    & |Location    | & |\x.Live John x                          | & in London & |ShortAnswer London Location| \\
Does John live in Paris? & |Bool        | & \makecell[l]{|\x.if x then (Live John Paris)| \\ |else Not (Live John Paris)|} & yes & |ShortAnswer True Bool| \\
What time is it?         & |Time        | & |\x.IsTime x                             | & It is 5am. & |Assert (IsTime 5.00)| \\\hline
Does John live in Paris? & |Prop -> Prop| & |\m. m (Live John Paris)                 | & yes & |ShortAnswer (\x. x)  (Prop -> Prop)| \\
Does John live in Paris? & |Prop -> Prop| & |\m. m (Live John Paris)                 | & from January & \makecell[l]{|ShortAnswer (\x. FromJanuary(x))|\\|(Prop -> Prop)|} \\\hline
Does John live in Paris? & \makecell[l]{|QuestionPolarity ->|\\|Prop -> Prop|} & |\m. m Positive (Live John Paris)| & yes & |ShortAnswer (\x. x)  (Prop -> Prop)| \\
Doesn't John live in Paris? & \makecell[l]{|QuestionPolarity ->|\\|Prop -> Prop|} & |\m. m Negative (Live John Paris)| & no / oui\footnote{is it so?} & \makecell[l]{|ShortAnswer (\ pol prop -> if Positive then|\\|Not prop else prop) (Prop -> Prop)|} \\
Doesn't John live in Paris? & \makecell[l]{|QuestionPolarity ->|\\|Prop -> Prop|} & |\m. m Negative (Live John Paris)| & si & \makecell[l]{|ShortAnswer (\ pol prop -> if Positive then ERROR|\\|else Not prop) (Prop -> Prop)|} \\
\end{tabular}
\caption{Examples of questions and the possible corresponding answers.\label{tbl:qa-ex}}
\end{sidewaystable}

Within the state of the agent, if the value of the requested answer is
represented as a metavariable |x|, then the question can be represented as: |Q A x (P x)|.

That is, the pending question is a triple of a type, a
metavariable |x|, and a proposition where |x| occurs.

We stress that |P x| is /not/ part of the information state of the
agent yet, rather the fact that the above question is /under
discussion/ is a fact. For example, after asking "when does John
live", we have:

#+BEGIN_code
haveQud : QUD (Q Location x (Live John x))
#+END_code

Resolving a question can be done by communicating an answer. An answer
to a question |(A : Type; P : A -> Prop)| can be of either of the two following forms: 
- ShortAnswer :: is a pair of an element |X:A| and its type |A|, represented as |ShortAnswer X A|
- Assertion :: is a proposition |P|, represented as |Assert P|



Therefore, one way to process a short answer is by the |processShort| rule:

#+BEGIN_code
processShort : ∀ x a p. ShortAnswer a x
             ⊸ QUD (Q x a p) ⊸ p
#+END_code

We demand in particular that types in the answer and in the question
match (|a| occurs in both places). Additionally, because |x| occurs in |p|, the information
state will mention the concrete |x| which was provided in the answer.

For example, if the QUD was |(Q Location x (Live John x))| and the
system processes the answer |ShortAnswer Location Paris|, then |x|
unifies with |Paris|, and the new state will include:
#+begin_code
Live John Paris
#+end_code

To process assertions, we can use the following rule:

#+BEGIN_code
processAssert  :  ∀ x a p. Assert p ⊸
                  QUD (Q x a p) ⊸ p
#+END_code

That is, (1) if |p| was asserted , (2) the proposition |q| is part of a question under discussion, and (3) p can be
unified with q, then the assertion resolves the
question. Additionally, the metavariable |x| is grounded to a concrete
value by virtue of unification of |p| and |q|. Examples:

"John lives in Paris" answers both questions "Where does John live"
and "Does John live in Paris" (there is unification), but, not, for
example "What time is it?" (there is no unification).

Note that, in both cases (|processAssert| and |processShort|), the
information state is updated with the proposition posed in the
question.

*** A bit more on polar questions
- Treatment of "no" (explain)

More on ellipsis?
- "he doesn't"
- "he does"


** Notion of unique and concrete answers

However, one should consider the question resolved only if the answer
is "unique". For example, the assertion "John lives somewhere" does
not resolve the question "where does John live". That is, if
"somewhere" is represented by a metavariable, then the answer is not
resolving.

Assume a two-place predicate |Eat| with agent as first argument and
object as second argument. The phrase "John eats an apple" could then
be represented as |Eat(John,Apple)|. According to our theory, one can
then represent the phrase "John eats" as |Eat(John,x)|, with |x| being
a metavariable.

Assume now a system with the state:

#+BEGIN_code
Eat(John,Apple)
#+END_code

Then the question "What does John eat", represented as |(Q Food x
(Eat(John,x)))|, can be answered.  From the point of view of modelling
with linear logic, we could attempt to model the answering by the
rule:

#+BEGIN_code
(a : Type) -> (x : a) -> (p : Prop)
-> QUD (Q a x p) -> p ⊸ (p ⊗ Answer x (Q x p))
#+END_code
Note: taking a linear argument and producing it again is a common
pattern, which can be spelled out |A ⊸ (A ⊗ P)|. It is so common that
from here on we use the syntactic sugar |A -* P| for it, so the above rule will be written:
#+BEGIN_code
(a : Type) -> (x : a) -> (p : Prop)
-> QUD (Q a x p) -> p -* Answer x (Q x p)
#+END_code

The above states that, if |x| makes the proposition |p| true (more
precisely, provable --- we require that |p| is a fact in the last
argument) then it is valid to answer |x| if |Q a x p| is under
discussion. However, there is an issue with the above rule: if |x| is
/not unique/, then one would not consider $x$ a suitable
answer. Indeed, assume instead that the system is in the state:

#+BEGIN_code
Eat(John,x)
#+END_code

Then the question cannot be answered, because |x| stands for some
unknown thing. The proper answer is then "I do not know".

Hence, we introduce another type-former |(x : A) !-> B|. As for |(x :
A) -> B|, it introduces the metavariable $x$. However, the rule fires
only when |x| is made /grounded/ (it is bound to a term which does not
contain any metavariable) and /unique/ by matching the rule. That is,
it won't match in the previous example, because the answer is not
grounded (it contains unknowns). Additionally, it won't match if the
state of the system is composed of the two
hypotheses |Eat(John,Apple)| and |Eat(John,Orange)|: the answer is not
unique.

Thus, the rule for answering can be written:

#+BEGIN_code
produceAnswer : (a : Type) -> (x : a) !-> (p : Prop)
-> QUD (Q a x p) -> p -* ShortAnswer x a
#+END_code

For example, if we have the following state:
#+BEGIN_code
QUD (Q Food x (Eat(John,x)))
Eat(John,Apple)
#+END_code

The system can unify |QUD (Q Food x (Eat(John,x)))| and |QUD (Q a x
p)|, yielding |a = Food|, |p=Eat(John,x)|. Then, we search for a
proof |p|, and to do this, it can unify |Eat(John,x)|
with |Eat(John,Apple)|, giving finally the answer |x=Apple| and
therefore the state becomes:
#+BEGIN_code
Eat(John,Apple)
ShortAnswer Apple Food
#+END_code

Note that the fact |Eat(John,Apple)| is found both as hypothesis and a
conclusion of |produceAnswer|, and therefore it is remains in the
information state.

** Clarification requests

In this section we discuss an alternative kind of answering, which is
to issue clarification requests.  To see how they can occur, consider
again the question "what does john eat", in the same information state
as above.
A proper answer could be "An apple and an orange" or "An apple or an
orange". However we consider here a third possibility: instead of
answering, the agent can issue a clarification request (TODO: is this
reasonable? When ... etc.) [fn:VM: maybe more intuitive example, e.g. with ’like’? like(john,bananas) like(john,dogs)]

To illustrate, consider the question "What is being eaten?"
represented as |Q x (Eat(y,x))|,  with the state
#+BEGIN_code
Eat(John,Apple)
Eat(Mary,Apple)
#+END_code
Then the agent can unambguously answer "An apple": even if we do not
know who we're talking about, it does not matter: only an apple is
being eaten. However, If the state is
#+BEGIN_code
Eat(John,Apple)
Eat(Mary,Orange)
#+END_code
Then, a probable answer would be a /clarification request/, namely
"By whom?".

To detect situations where a clarification request can be issued, we can use the following rule:
#+BEGIN_code
(a : Type) -> (x : a) ?-> (p : Prop)
   -> QUD (Q x p) -> p -* CR
#+END_code
(We leave the exact form of the CR abstract for now and come back to it below)

The conditions are similar to that of the answering rule. The
principal difference is the use of the |?->| operator, which conditions
on a metavariable which remains not (fully) grounded, or which can be
unified to several ground terms --- the opposite of the |!->| operator.

We can then turn our attention to the formulation of this clarification request.
It is itself a question, and has a tricky representation:

#+BEGIN_code
Q Person z (z = y)
#+END_code

That is, the question is asking about some aspect which was left
implicit in the original question (what is being eaten). In our terms,
it must refer to the metavariable (|y|) which the original
question included.  After getting an answer, (say |Mary|), |z|
will be bound to a ground term, and, in turn, the fact |z=y| will
ensure that |y| becomes ground. 

#+BEGIN_code
Eat(John,Apple)
Eat(Mary,Orange)
ori  ::  QUD (Q Food x (Eat(y,x)))
cr   ::  QUD (Q Person z (z=y))
a    ::  ShortAnswer Mary Person
#+END_code
after applying |processShort|:
#+BEGIN_code
Eat(John,Apple)
Eat(Mary,Orange)
ori :: QUD (Q Food x (Eat(y,x)))
r ::  Mary=y
#+END_code


This means the original question will, by unification, become |Q Food
x (Eat(Mary,x))|, and it can be unambiguously answered using the
/canAnswer/ rule. We note that the logical form of the question (|z|
such that |z=y|) is typically realised in a complicated way. In our
example, it could be "By whom"; echoing part of the original question
and assuming cooperative communication so that the questioner properly
relates the clarification request to the implicits of the original
questions. (In sec. ref:sec:bus)

In practice, the form of clarification questions will greatly vary
depending on the context.

The above suposes a clear-cut distinction: if an answer is unique, it
is given; otherwise a clarification request is issued. However,
answers could simply be exhaustive ("An apple or an orange").  If the
original questioners are unhappy with the ambiguity, they are free to
issue more precise questions. In practice, one can easily imagine an
ambiguity threshold after which clarification requests are
preferred. In the simplest form, this ambiguity threshold could be
expressed by the length of the answer. In our example, if one has to
list, say, 20 different kinds of food, it is easy to imagine that the
answer won't be fully given. In fact, this question can be the topic
of an experimental study.

*** More on CRs
TODO: flow sentence.

Consider the exchange:

#+BEGIN_quote
A: Where does John live? \\
B: Do you mean while he is in confinement?
#+END_quote

In the above, there is an (implicit) extra argument to the |Live|
predicate, corresponding to, say, a time
interval: |Live(who,location,confinement)|.

However most of the time one may choose to leave this parameter
implicit. This is what is done for example when asking the above
question:

#+BEGIN_code
Q Location x Live(John,x,y)
#+END_code
assuming a metavariable y of type |Bool|.

If the question can be answered without regard for whether there is
confinement or not, then the metavariable will remain free for the
duration of the dialogue. If on the other hand, answering the question
demands clarification, this can be done using the mechanisms described
above.

In sum, in our model, to support clarification requests, a system must
integrate many arguments and use metavariables.

*** Even more on CRs

Consider now the closed question "Does John live in Paris?". The
questionee may decide that there is some ambiguity about /which/ Paris
one is talking about --- after all there are several places called
like this. To be able to model this, the |Live| relation needs to be
generalised to be a 3-place predicate, where the country is specified.

We can then assume that the question can be encoded for simplicity
as |\x. if x then (Live John Paris y) else Not (Live John Paris y)|.
That is, the country is implicit and represented by a metavariable.

If the system has the following facts:
#+begin_code
Live John Paris France
Not (Live John Paris Denmark)
#+end_code
then both "True" and "False" are valid answers, and a clarification
requests should be issued: |Q Country z (z=y)|. We see again that the
realisation of the clarification request depends highly on the
formulation of the question and the context. In this case "Do you mean
Paris, France?"  would be suitable.

*** Alternative representation with binary predicates only

The above presentation (using a ternary predicate) is perhaps not
ideal. (Because the country is functionally dependent on the location,
these two concepts should be linked directly together rather than
involve the |Live| predicate.) Using an intermediary entity type for
locations and binary predicates, one can represent the question "Does
John live in Paris?" as follows: |\x. if x then (Live John y -> Name y
Paris) else Not (Live John y -> Name y Paris) |

Literally, "Does John live in a place called Paris?".

The ambiguity of the |Paris| name can be represented by several
locations named |Paris|, |X| and |Y| in our illustration:
#+begin_code
Name Paris X
Name Paris Y
Live John X
Not (Live John Y)
Country France X
Not (Country France Y)
#+end_code

Because John lives in |X| but not in |Y| the question is
ambiguous. One way to lift the ambiguity is raise the clarification
request as above. Here it can be phrased as a boolean question
again: |Q Bool (\x. if x then Country France y else Not (Country
France y))|


** Implementation notes
TODO: move earlier

In our implementation, the information state is represented by a set
we treat the information state as a multiset [?] of /linear
hypotheses/ that can be queried. Because they are linear, these
hypotheses can also be removed from the state.

On top of this, we have set of immuable rules (they remain available
even after being used).

** TODO COMMENT Extras

As an example, we can show how the rule for /QUD-incrementation/ from
cite:ginzburg2012interactive can be formulated in this terms. Here
we consider the dialogue between interlocutors /A/ and /B/, when /A/ asks
/B/[fn::Here we omit addressees as the conversation is only two-party.]
a question /Q/. The question /Q/ just have been posed and therefore has
appeared on the DGBs of both /A/ and /B/ as the latest ~Ask~ move
(~LatestMove~).
#+BEGIN_SRC sh :exports code
-- context
_ :: DGB A (LatestMove (Ask A Q));
_ :: DGB B (LatestMove (Ask A Q));
#+END_SRC

Now we can define our update rule that act on the contextual resources:
#+BEGIN_SRC
_ : (q : Question) -> (x y : User) ->
    DGB x (LatestMove (Ask y q)) ⊸ DGB x (QUD q);
#+END_SRC
Here, for any interlocutor, her ~LatestMove~ asking a question is
consumed and her ~QUD~ is updated with the question from the ~Ask~ move.

* TODO Implementation: show actual set of rules

* Application: Bus example
label:sec:bus
- Formalisation of all the aspects of the the example in the theory.
- Say it's a complete and implemented system

* Related work

Use the same basic idea as cite:dixon2009plans. But we have the additional unicity
operators (~X !-> Y~) and (~X ?-> Y~). Also we explicitly deal with QA and CR.

* Kos-inspired dialogue management with linear logic
- Connect with AixDial paper.
- image with basic SDS architecture

- importantly each (LL) rule manipulates a part of the information
  state (captured by its premisses) and leaves everything else in the
  state alone. (This is what TTR tries to do with its "assymetric
  merge" operation).

- Additionally, the use of metavariables is new.

** Domain-independent rules

*** Interface with language understanding and generation
Here we assume that the information that comes from a source which is
external to the dialogue manager is expressed in terms of semantic
interpretations of moves, and contains information about the speaker
and the addressee in a structured way. Here we provide 5 basic types
of moves as an illustration:
#+BEGIN_code
Greet            spkr  addr
CounterGreet     spkr  addr
Ask           q  spkr  addr
ShortAnswer   v  spkr  addr
Assert        p  spkr  addr
#+END_code

These moves can either be received as input or produced as outputs. If
they are inputs, they come from the NLU component, and they enter the
context with |Heard| predicate. For example, if one hears a greeting,
the proposition |Heard (Greet S A)| is added to the context, without
any rule being fired --- this is what we mean by an external source.


If they are outputs, to be further used by the NLG component, some
rule will place them in |Agenda|. For example, to issue a
countergreeting, a rule will place the proposition |Agenda
(CounterGreet A S)| in the information state.

As it is easily noticed, each move is accompanied by the information
about who has uttered it, and towards whom was it addressed. All the
moves are stacked in the |Moves| part of the participant’s dialogue
gameboard[fn::VM:should we get DGB back?] using the following rule.
#+BEGIN_code
pushMove : (m : Move) -> (ms : List Move) ->
           PushMove m ⊸ Moves ms ⊸ Moves (Cons m ms);
#+END_code

*** Initial state
In general, we start with empty |QUD| and |Agenda|. An empty |QUD| can
be adjusted if in a certain domain some open questions are assumed
from the start. The |Agenda| might not be empty if one would want the
system to initiate the conversation. There are also no moves: nothing
has been said by neither party.

#+BEGIN_code
_ :: QUD Nil;
_ :: Agenda Nil;
_ :: Moves Nil;
#+END_code

*** Hearing
The capacity of "hearing" or, in other words, starting the processing
of semantic representations of utterances from the NLU component, is
implemented with the following rule:
#+BEGIN_code
hearAndRemember  :
  (m : DP -> DP -> Move) -> (x y : DP) ->
  Heard (m x y) ⊸
  HasTurn x     ⊸
  [_ :: PushMove (m x y) ;
   _ :: HasTurn y ;
   _ :: Agenda Nil];
#+END_code
where |(m x y)| is a semantic representation of the utterance. Here it
is assumed that participant |x| has a turn and, as a result, turn was
taken by her partner |y|. We do here several things: 1. place the move
in a move list for further references (|PushMove|) 2. record the
turn-switching (which in a complete system may not apply to all cases
--- then additional hypotheses would be added.) 3. Prepare to react by
recording that agenda is empty.

*** Uttering
The capacity of "uttering" represents an ability to generate
information for the NLG component. NLP component is represented
by |Agenda| that contains a move that is just about to be uttered.

#+BEGIN_code
utterAndRemember :
  (m : DP -> DP -> Move) ->
  (x y : DP) ->
  Agenda (m x y) ⊸
  HasTurn x ⊸ [_ :: Utter (m x y); _ :: PushMove (m x y) ; _ :: HasTurn y];
#+END_code

Here also, we take care of turn-taking in the same rule.
*** Basic adjacency: greeting
We can show how basic move adjacency can be defined in the example of
countergreeting preconditioned by a greeting from the other party:

#+BEGIN_code
counterGreeting : (x y : DP) -> (ms : List Move) ->
                  HasTurn x -*
                  Moves (Cons (Greet y x) ms)  -*
                  Agenda Nil ⊸
                  [_ :: Agenda (CounterGreet x y)];
#+END_code
*** QUD identification
#+BEGIN_code
setQUD :
   (ms : List Move) ->
   (q : Question) ->
   (x y : DP) ->
   Moves (Cons (Ask q x y) ms) -* QUD Nil -o QUD q;
#+END_code

*** Question resolution
Variant of |produceAnswer| rule that works as part of dialogue management

#+BEGIN_code
produceAnswer :
   (a : Type) ->
   (x : a) !->
   (p : Prop) ->
   QUD (Q a x p) -o
   p -*
   Agenda Nil -o
   [_ :: Agenda (ShortAnswer x a SYSTEM USER); 
    _ :: Answered (Q a x p)];
#+END_code


*** Note on turn taking
- how can be turned into something more advanced
** Example
With/without  system-cr:
- greeting exchange
- when there is a bus from A?
- (towards where? 
- B)
- it is at T

With user-cr:
- ??? 

* Evaluation/Discussion/Future work
- discussing the corrections
- discuss the clarification requests in a more specific way: we can always redefine the referent
- in dialogue systems meta-variables are always subject to clarification and correction (substitution)
- dependencies between questions (who killed bill -> who was around?)

- clarification could be narrowing from general types to subtypes.




\bibliography{tal}

* COMMENT references
bibliography:tal.bib



# Local Variables:
# org-latex-subtitle-separate: t
# org-latex-classes: (("article-hermes_french" "\\documentclass[english,utf8]{article-hermes_french} " ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")("\\subsubsection{%s}" . "\\subsubsection*{%s}") ("\\paragraph{%s}" . "\\paragraph*{%s}") ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# End:

* COMMENT notes 

** VM & JG <2020-06-12 Fri>
Why should we care? 
- one of the ideas is to deal with structured NLU representations
- repair is a minor issue
- reach coherence, and some responses have low frequency, therefore it
  is hard to learn them from data

How is it better than other systems?
- Traum: ICT systems, sensai, psychotheraphy consulting
- TDM
- end2end, as they referee sigdial/acl
- Young et al.
- Sadek, Phil Colin

More punch: either benefit for semantic theories, or to dialogue system building.

+ Shalom’s point from Friday: formal systems as reality/sanity check,
  can be used to highlight linguistic phenomena and relations between
  them. A source of insight for improving deep learning systems. 

